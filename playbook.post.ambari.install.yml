---
- hosts: all
  remote_user: root
  tasks:
    - name: Make libcrypto available to hadoop native
      file: src=/lib/x86_64-linux-gnu/libcrypto.so.1.0.0 dest=/lib/x86_64-linux-gnu/libcrypto.so state=link
    - name: Check hadoop native
      command: hadoop checknative -a
    - name: Allow yarn user to run docker containers
      user: name=yarn append=yes groups=docker
      notify:
        - restart yarn node managers
    - name: Posix users
      user: user={{ item.name }} group=users groups=docker shell=/bin/bash uid={{ item.uid }} password='{{ item.password }}'
      with_items: "{{ users }}"
    - name: Posix users loads hadoop/spark env on login
      with_items: "{{ users }}"
      blockinfile:
        dest: /home/{{ item.name }}/.bashrc
        owner: '{{ item.name }}'
        block: |
          . /etc/hadoop/conf/hadoop-env.sh
          . /etc/hadoop/conf/yarn-env.sh
          . /etc/hadoop/conf/mapred-env.sh
          export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
          export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
          export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
          . /usr/hdp/current/spark-client/conf/spark-env.sh
          . /usr/hdp/current/hive-client/conf/hive-env.sh
- hosts: frontend
  remote_user: root
  vars:
      spark_assembly: /usr/hdp/current/spark-client/lib/spark-assembly-1.6.0.2.4.0.0-169-hadoop2.7.1.2.4.0.0-169.jar
      spark_on_hdfs: /hdp/apps/2.4.0.0-169/spark/spark-hdp-assembly.jar
      cran_url: http://cran-mirror.cs.uu.nl
  tasks:
    - name: sparkR in PATH
      file: src=/usr/hdp/current/spark-client/bin/sparkR dest=/usr/bin/sparkR state=link
    - name: Check shelly user has hdfs home dir
      command: hdfs dfs -test -e /user/{{ item.name }}
      become: yes
      become_user: hdfs
      ignore_errors: True
      register: hdfs_home_exists
      with_items: "{{ users }}"
      changed_when: False
    - name: hdfs home dir
      command: hdfs dfs -mkdir /user/{{ item.item.name }}
      become: yes
      become_user: hdfs
      with_items: "{{ hdfs_home_exists.results }}"
      when: item|failed
    - name: Shelly owns hdfs home dir
      command: hdfs dfs -chown shelly /user/{{ item.item.name }}
      become: yes
      become_user: hdfs
      with_items: "{{ hdfs_home_exists.results }}"
      when: item|failed
    - name: Check spark assembly on HDFS
      command: hdfs dfs -test -e {{ spark_on_hdfs }}
      become: yes
      become_user: hdfs
      ignore_errors: True
      register: spark_assemlby_on_hdfs_exists
      changed_when: False
    - name: Spark app dir on HDFS
      command: hdfs dfs -mkdir /hdp/apps/2.4.0.0-169/spark
      become: yes
      become_user: hdfs
      when: spark_assemlby_on_hdfs_exists|failed
    - name: Copy spark assembly to HDFS
      command: hdfs dfs -copyFromLocal {{ spark_assembly }} {{ spark_on_hdfs }}
      become: yes
      become_user: hdfs
      when: spark_assemlby_on_hdfs_exists|failed
    - name: Deps 4 JupyterHub notebook
      apt: name={{ item }}
      with_items:
        - npm
        - nodejs
        - nodejs-legacy
        - python3-pip
        - python-pip
    - name: configurable-http-proxy
      npm: name=configurable-http-proxy global=yes
    - name: Jupyter for Python 3
      pip: name=jupyter executable=pip3
    - name: Toree (Spark Jupyter)
      pip: name=toree executable=pip extra_args='--pre'
    - name: Toree to Jupyter
      command: jupyter toree install --spark_home=/usr/hdp/current/spark-client creates=/usr/local/share/jupyter/kernels/toree
    # - name: Py Toree to Jupyter
    #   command: jupyter toree install --spark_home=/usr/hdp/current/spark-client --kernel_name=pytoree creates=/usr/local/share/jupyter/kernels/pytoree
    # https://github.com/ibm-et/spark-kernel/wiki/Language-Support-on-the-Spark-Kernel
    # - name: Py Toree kernel
    #   copy: src=pytoree.kernel.json dest=/usr/local/share/jupyter/kernels/pytoree/kernel.json
    # - name: R Toree to Jupyter
    #   command: jupyter toree install --spark_home=/usr/hdp/current/spark-client --kernel_name=rtoree creates=/usr/local/share/jupyter/kernels/rtoree
    - name: JupyterHub
      pip: name=jupyterhub executable=pip3
    - name: /etc/jupyterhub
      file: path=/etc/jupyterhub state=directory
    - name: JupyterHub config
      command: jupyterhub --generate-config -f /etc/jupyterhub/jupyterhub.py creates=/etc/jupyterhub/jupyterhub.py
    - name: Bind JupyterHub to all interfaces
      blockinfile:
        dest: /etc/jupyterhub/jupyterhub.py
        block: |
          c.JupyterHub.ip = '0.0.0.0'
    - name: Start/stop script JupyterHub
      copy: src=jupyterhub.upstart dest=/etc/init/jupyterhub.conf
    - name: JupyterHub started
      service: name=jupyterhub state=started

    # - name: Plyr.spark deps deps
    #   apt: pkg={{ item}}
    #   with_items:
    #     - libssl-dev
    #     - libcurl4-openssl-dev
    #     - libssh2-1-dev
    #     - r-cran-littler
    #     - r-cran-stringr
    #     - r-cran-dbi
    # - name: Install docopt R package
    #   shell: echo 'install.packages(c("docopt"), repos="{{ cran_url }}")' | R --vanilla
    #   args:
    #     creates: /usr/local/lib/R/site-library/docopt
    # - name: R javareconf
    #   command: R CMD javareconf
    # - name: Plyr.spark deps (https://github.com/RevolutionAnalytics/dplyr-spark)
    #   command: /usr/lib/R/site-library/littler/examples/install2.r -r {{ cran_url }} {{ item }} creates=/usr/local/lib/R/site-library/{{ item }}
    #   with_items:
    #     - RJDBC
    #     - devtools
    # - name: Plyr.spark
    #   shell: echo 'devtools::install_github("RevolutionAnalytics/dplyr-spark@0.3.0", subdir = "pkg")' | R --vanilla
    #   args:
    #     creates: /usr/local/lib/R/site-library/dplyr.spark

# TODO
# * Restart yarn using ambari api when its added to docker group, https://cwiki.apache.org/confluence/display/AMBARI/Restarting+host+components+via+the+API
# * Add R plyr spark
