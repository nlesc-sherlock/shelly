---
- hosts: spark
  remote_user: '{{ remote_user }}'
  tasks:
    - name: Make libcrypto available to hadoop native
      file: src=/lib/x86_64-linux-gnu/libcrypto.so.1.0.0 dest=/lib/x86_64-linux-gnu/libcrypto.so state=link
    - name: Check hadoop native
      command: hadoop checknative -a
    - name: Allow yarn user to run docker containers
      user: name=yarn append=yes groups=docker
    - name: Posix users
      user: user={{ item.name }} group=users groups=docker shell=/bin/bash uid={{ item.uid }} password='{{ item.password }}'
      with_items: "{{ users }}"
    - name: Posix users loads hadoop/spark env on login
      with_items: "{{ users }}"
      blockinfile:
        dest: /home/{{ item.name }}/.bashrc
        owner: '{{ item.name }}'
        block: |
          . /etc/hadoop/conf/hadoop-env.sh
          . /etc/hadoop/conf/yarn-env.sh
          . /etc/hadoop/conf/mapred-env.sh
          export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
          export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
          export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
          . /usr/hdp/current/spark2-client/conf/spark-env.sh
          . /usr/hdp/current/hive-client/conf/hive-env.sh
- hosts: spark-frontend
  remote_user: '{{ remote_user }}'
  vars:
      cran_url: http://cran-mirror.cs.uu.nl
  tasks:
    - name: Check shelly user has hdfs home dir
      command: hdfs dfs -test -e /user/{{ item.name }}
      become: yes
      become_user: hdfs
      ignore_errors: True
      register: hdfs_home_exists
      with_items: "{{ users }}"
      changed_when: False
    - name: hdfs home dir
      command: hdfs dfs -mkdir /user/{{ item.item.name }}
      become: yes
      become_user: hdfs
      with_items: "{{ hdfs_home_exists.results }}"
      when: item|failed
    - name: Shelly owns hdfs home dir
      command: hdfs dfs -chown shelly /user/{{ item.item.name }}
      become: yes
      become_user: hdfs
      with_items: "{{ hdfs_home_exists.results }}"
      when: item|failed
    - name: Deps 4 JupyterHub notebook
      apt: name={{ item }}
      with_items:
        - npm
        - nodejs
        - nodejs-legacy
        - python3-pip
        - python-pip
        - libffi-dev
    - name: configurable-http-proxy
      npm: name=configurable-http-proxy global=yes
    # - name: Repair pip3
    #   shell: apt-get purge -y python3-pip && curl https://bootstrap.pypa.io/get-pip.py && python3 ./get-pip.py && apt-get install python3-pip
    - name: Jupyter for Python 3
      pip: name=jupyter executable=pip3
    - name: Toree (Spark Jupyter)
      pip: name=toree executable=pip extra_args='--pre'
    - name: Toree to Jupyter
      command: jupyter toree install --spark_home=/usr/hdp/current/spark2-client creates=/usr/local/share/jupyter/kernels/apache_toree_scala
    - name: Py Toree to Jupyter
      command: jupyter toree install --spark_home=/usr/hdp/current/spark2-client --interpreters=PySpark creates=/usr/local/share/jupyter/kernels/apache_toree_pyspark
    - name: SQL Toree to Jupyter
      command: jupyter toree install --spark_home=/usr/hdp/current/spark2-client --interpreters=SQL creates=/usr/local/share/jupyter/kernels/apache_toree_sql
    - name: SparkR Toree to Jupyter
      command: jupyter toree install --spark_home=/usr/hdp/current/spark2-client --interpreters=SparkR creates=/usr/local/share/jupyter/kernels/apache_toree_sparkr
    - name: ipykernel for Python 2
      pip: name=ipykernel executable=pip
    - name: Python 2 to Jupyter
      shell: /usr/bin/python2 -m ipykernel install creates=/usr/local/share/jupyter/kernels/python2
    - name: JupyterHub
      pip: name=jupyterhub executable=pip3
    - name: /etc/jupyterhub
      file: path=/etc/jupyterhub state=directory
    - name: JupyterHub config
      command: jupyterhub --generate-config -f /etc/jupyterhub/jupyterhub.py creates=/etc/jupyterhub/jupyterhub.py
    - name: Bind JupyterHub to all interfaces
      blockinfile:
        dest: /etc/jupyterhub/jupyterhub.py
        block: |
          c.JupyterHub.ip = '0.0.0.0'
    - name: Start/stop script JupyterHub
      copy: src=jupyterhub.upstart dest=/etc/init/jupyterhub.conf
    - name: JupyterHub started
      service: name=jupyterhub state=started
    - name: Admin user has hdfs home dir
      command: hdfs dfs -mkdir /user/admin
      become: yes
      become_user: hdfs
      ignore_errors: True
    - name: Admin user owns hdfs home dir
      command: hdfs dfs -chown admin:hadoop /user/admin
      become: yes
      become_user: hdfs
      ignore_errors: True
    - name: Root user has hdfs home dir
      command: hdfs dfs -mkdir /user/root
      become: yes
      become_user: hdfs
      ignore_errors: True
    - name: Root user owns hdfs home dir
      command: hdfs dfs -chown root:hadoop /user/admin
      become: yes
      become_user: hdfs
      ignore_errors: True
    - name: Findspark
      pip: name=findspark executable=pip3
    - name: Hadoop config 2 local
      fetch: src=/etc/hadoop/conf/{{ item }} dest=shelly-conf/hadoop/ flat=yes
      with_items:
        - capacity-scheduler.xml
        - core-site.xml
        - hadoop-env.sh
        - hdfs-site.xml
        - log4j.properties
        - yarn-env.sh
        - yarn-site.xml
        - topology_mappings.data
        - topology_script.py
    - name: Spark config 2 local
      fetch: src=/etc/spark2/conf/{{ item }} dest=shelly-conf/spark/ flat=yes
      with_items:
        - spark-defaults.conf
        - spark-env.sh
        - metrics.properties
        - log4j.properties
    # - name: Plyr.spark deps deps
    #   apt: pkg={{ item}}
    #   with_items:
    #     - libssl-dev
    #     - libcurl4-openssl-dev
    #     - libssh2-1-dev
    #     - r-cran-littler
    #     - r-cran-stringr
    #     - r-cran-dbi
    # - name: Install docopt R package
    #   shell: echo 'install.packages(c("docopt"), repos="{{ cran_url }}")' | R --vanilla
    #   args:
    #     creates: /usr/local/lib/R/site-library/docopt
    # - name: R javareconf
    #   command: R CMD javareconf
    # - name: Plyr.spark deps (https://github.com/RevolutionAnalytics/dplyr-spark)
    #   command: /usr/lib/R/site-library/littler/examples/install2.r -r {{ cran_url }} {{ item }} creates=/usr/local/lib/R/site-library/{{ item }}
    #   with_items:
    #     - RJDBC
    #     - devtools
    # - name: Plyr.spark
    #   shell: echo 'devtools::install_github("RevolutionAnalytics/dplyr-spark@0.3.0", subdir = "pkg")' | R --vanilla
    #   args:
    #     creates: /usr/local/lib/R/site-library/dplyr.spark

# TODO
# * Restart yarn using ambari api when its added to docker group, https://cwiki.apache.org/confluence/display/AMBARI/Restarting+host+components+via+the+API
# * Add R plyr spark
